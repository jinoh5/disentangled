{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinoh5/disentangled/blob/main/run_multi_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sdIM7UVnyZqo"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBSun9f9zqtP"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p5haXXz6zsf0"
      },
      "outputs": [],
      "source": [
        "sigma = 1\n",
        "N = 20 # 20 dimensional space\n",
        "T = 100\n",
        "numCond = 4\n",
        "noise_var = 2 # may increase this to 2\n",
        "\n",
        "input_dim = 20\n",
        "output_dim = 2\n",
        "learning_rate = 0.001\n",
        "batch_size = 200\n",
        "hidden_dim = input_dim * 2\n",
        "exp_num = 3\n",
        "num_epochs = 1000\n",
        "iter_num = 5\n",
        "\n",
        "beta1 = 0.0001\n",
        "beta2 = 0\n",
        "beta3 = 100\n",
        "beta4 = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgC8jGC7yytY"
      },
      "source": [
        "## INPUT set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wr2w_oaMyx5G"
      },
      "outputs": [],
      "source": [
        "# # Disentangled Points (Either comment these)\n",
        "# cond = np.zeros((4,2))\n",
        "# cond[0,:] = [1,1]\n",
        "# cond[1,:] = [-1,1]\n",
        "# cond[2,:] = [-1,-1]\n",
        "# cond[3,:] = [1,-1]\n",
        "\n",
        "# cond = cond.T\n",
        "\n",
        "# # Project the data onto the 20-D space\n",
        "# projection = np.zeros((numCond,T,N))\n",
        "# W = stats.norm.rvs(loc=0, scale=sigma, size=[N,2])\n",
        "# for i in range(T): # T = 100 trials\n",
        "#     projection[0,i,:] = W @ cond[:,0] # V (N x 1) = W (N x 2) * I (2 x 1)\n",
        "#     projection[1,i,:] = W @ cond[:,1]\n",
        "#     projection[2,i,:] = W @ cond[:,2]\n",
        "#     projection[3,i,:] = W @ cond[:,3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entangled Input (Either comment these)\n",
        "subInput_non = np.random.normal(loc=0, scale=sigma, size=[numCond,N])\n",
        "ent_input = np.zeros((numCond,T,N))\n",
        "\n",
        "for i in range(4):\n",
        "    ent_input[i,:,:] = np.tile(subInput_non[i,:], (T, 1))\n",
        "\n",
        "projection = ent_input"
      ],
      "metadata": {
        "id": "YKKHqiENUc2F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU7dxFGNy1Sc"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WzkMgqFuy2cU"
      },
      "outputs": [],
      "source": [
        "def create_input(projection, noise_var):\n",
        "  # Create noise\n",
        "  noise_matrix = np.random.normal(loc=0, scale=noise_var, size=projection.shape)\n",
        "\n",
        "  # Add noise\n",
        "  input_w_noise = projection + noise_matrix\n",
        "\n",
        "  # Reshape to create 400 x 20\n",
        "  input = np.vstack((input_w_noise[0,:,:], input_w_noise[1,:,:], input_w_noise[2,:,:], input_w_noise[3,:,:]))\n",
        "\n",
        "  # Experiment conditions\n",
        "  org_cond = np.zeros((4,2))\n",
        "  org_cond[1,:] = [1,0]\n",
        "  org_cond[2,:] = [1,1]\n",
        "  org_cond[3,:] = [0,1]\n",
        "\n",
        "  # original task\n",
        "  task1 = org_cond[:,0]\n",
        "  task2 = org_cond[:,1]\n",
        "  xor = np.sum(org_cond,axis=1)%2\n",
        "\n",
        "  # Classes\n",
        "  task1_class = np.repeat(task1, T)\n",
        "  task2_class = np.repeat(task2, T)\n",
        "  xor_class = np.repeat(xor, T)\n",
        "\n",
        "  classes = np.zeros((numCond*T, 3))\n",
        "  classes[:,0] = task1_class\n",
        "  classes[:,1] = task2_class\n",
        "  classes[:,2] = xor_class\n",
        "\n",
        "  return input, classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dCZmLoj23qJD"
      },
      "outputs": [],
      "source": [
        "# Losses\n",
        "def custom_loss(output, targets, sparsity, beta1, beta2, PR, beta3, PR_connect, beta4):\n",
        "  '''\n",
        "  Custom loss function\n",
        "  '''\n",
        "  criterion = nn.CrossEntropyLoss() # first task\n",
        "  crossEntropy = criterion(output, targets)\n",
        "\n",
        "  totalLoss = beta1 * criterion(output, targets) + beta2 * sparsity + beta3 * PR + beta4 * PR_connect\n",
        "  return crossEntropy, totalLoss\n",
        "\n",
        "def L1_norm(hidden_output):\n",
        "  l1_norm = torch.mean(torch.abs(hidden_output)) # gives me a simple number\n",
        "  return l1_norm\n",
        "\n",
        "def PR_norm(hidden_output):\n",
        "  cov_matrix = torch.cov(hidden_output.T)\n",
        "  eigval, _ = torch.linalg.eigh(cov_matrix)\n",
        "  numerator = torch.sum(eigval) ** 2\n",
        "  denominator = torch.sum(eigval ** 2)\n",
        "  participation_ratio = numerator / denominator\n",
        "  return participation_ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jm7Mb6964m4p"
      },
      "outputs": [],
      "source": [
        "# Model (nonlinear classifier)\n",
        "class Simple_Nonlin(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(Simple_Nonlin, self).__init__()\n",
        "    self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
        "    self.hidden_layer = nn.Linear(hidden_dim, output_dim) # here, I can add noise to make noises in the hidden layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_h = F.relu(self.input_layer(x)) # Applying ReLU activation after input_layer\n",
        "    output = self.hidden_layer(x_h)\n",
        "    return x_h, output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x_NC0SeIyh-3"
      },
      "outputs": [],
      "source": [
        "def calculate_centroid(points):\n",
        "   # INPUT: PCAed 3D points 100 x 3 so that it can be 1 x 3\n",
        "    centroid_x = np.mean(points[:, 0])\n",
        "    centroid_y = np.mean(points[:, 1])\n",
        "    centroid_z = np.mean(points[:, 2])\n",
        "\n",
        "    return [centroid_x, centroid_y, centroid_z]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvzaCf505Iac"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-nca7QM22ECi"
      },
      "outputs": [],
      "source": [
        "train_input, train_classes = create_input(projection, noise_var)\n",
        "test_input, train_classes = create_input(projection, noise_var)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0-nd7kGQ5Xad"
      },
      "outputs": [],
      "source": [
        "trainI = torch.tensor(train_input, dtype=torch.float32)\n",
        "trainT = torch.tensor(train_classes, dtype=torch.long)\n",
        "\n",
        "testI = torch.tensor(test_input, dtype = torch.float32)\n",
        "testT = torch.tensor(train_classes, dtype = torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "g4fCfyqPIIKQ"
      },
      "outputs": [],
      "source": [
        "exp_list = [[0,0],\n",
        "            [0,1],\n",
        "            [0,2],\n",
        "            [1,0],\n",
        "            [1,1],\n",
        "            [1,2],\n",
        "            [2,0],\n",
        "            [2,1],\n",
        "            [2,2]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ADTDVfWcrbqU"
      },
      "outputs": [],
      "source": [
        "pca_dict = {i: np.zeros((iter_num, 400, 3)) for i in range(len(exp_list))}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca_dict[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2poriGiQKRW",
        "outputId": "e2c7bb2c-5d93-4f8d-f855-9391dd277901"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 400, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIIV3LnbnKYz",
        "outputId": "c4ccce5d-2e4f-4419-bad6-b602c2d6d8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0\n",
            "train # 0\n",
            "test # 0\n",
            "train # 0\n",
            "test # 1\n",
            "Stopping training at epoch 100 with avg_crossEntropy_loss 0.449157\n",
            "train # 0\n",
            "test # 2\n",
            "train # 1\n",
            "test # 0\n",
            "Stopping training at epoch 100 with avg_crossEntropy_loss 0.430839\n",
            "train # 1\n",
            "test # 1\n",
            "Stopping training at epoch 100 with avg_crossEntropy_loss 0.482184\n",
            "train # 1\n",
            "test # 2\n",
            "Stopping training at epoch 100 with avg_crossEntropy_loss 0.488586\n",
            "train # 2\n",
            "test # 0\n"
          ]
        }
      ],
      "source": [
        "# Create shuffled indices\n",
        "indices = torch.randperm(len(trainI))\n",
        "\n",
        "# Shuffled train and test\n",
        "shuffled_trainI = trainI[indices,:]\n",
        "shuffled_trainT = trainT[indices,:]\n",
        "\n",
        "shuffled_testI = testI[indices,:]\n",
        "shuffled_testT = testT[indices,:]\n",
        "\n",
        "total_accuracies = np.zeros((iter_num,3,3)) # you need iternum\n",
        "\n",
        "for iter in range(iter_num): # 1. LOOP FOR ITERATION OF THE EXPERIMENT\n",
        "  print(\"iter\", iter)\n",
        "  start_time = time.time()\n",
        "\n",
        "  for i in range(len(exp_list)): # 2. LOOP FOR EVERY EXPERIMENTAL CONDITION\n",
        "    exp_trainT = shuffled_trainT[:, exp_list[i][0]]\n",
        "    print(\"train #\", exp_list[i][0])\n",
        "\n",
        "    # Only grab the appropriate experiment condition\n",
        "    exp_testT = shuffled_testT[:, exp_list[i][1]]\n",
        "    print(\"test #\", exp_list[i][1])\n",
        "\n",
        "    # Create Tensor Dataset\n",
        "    train_dataset = TensorDataset(shuffled_trainI, exp_trainT)\n",
        "    test_dataset = TensorDataset(shuffled_testI, exp_testT)\n",
        "\n",
        "    # Create Train loader\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)\n",
        "\n",
        "    # Instantiate the model\n",
        "    model = Simple_Nonlin(input_dim, hidden_dim, output_dim) # so that you can only input 'model' not 'modelName'\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "    # All Losses Collection\n",
        "    trainLosses = []\n",
        "    valLosses = []\n",
        "    accuracies = []\n",
        "    sparsities = []\n",
        "    crossEntropyLosses = []\n",
        "    PR_losses = []\n",
        "    PR_connect_losses = []\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs): # 3. LOOP FOR EPOCH / COND\n",
        "\n",
        "      # Training Stage\n",
        "      model.train()\n",
        "\n",
        "      # losses\n",
        "      train_loss = 0\n",
        "      sparsity_loss = 0\n",
        "      crossEntropy_loss = 0\n",
        "      PR_loss = 0\n",
        "      PR_connect_loss = 0\n",
        "\n",
        "      for train_input, train_target in train_loader: # 4. LOOP FOR TRAIN BATCH\n",
        "\n",
        "        train_input = train_input.float()\n",
        "\n",
        "        # clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Apply the model\n",
        "        x_h, train_output = model(train_input)\n",
        "\n",
        "        # Take out the connectivity matrix\n",
        "        connectivity_matrix = model.input_layer.weight.data\n",
        "        PR_connect = PR_norm(connectivity_matrix)\n",
        "        PR_connect_loss += PR_connect\n",
        "\n",
        "        # Compute sparsity from the hidden layer\n",
        "        sparsity = L1_norm(x_h)\n",
        "        sparsity_loss += sparsity\n",
        "\n",
        "        # Compute participation ratio from the hidden layer\n",
        "        PR = PR_norm(x_h)\n",
        "        PR_loss += PR\n",
        "\n",
        "        # <<Compute the total loss>>\n",
        "        CE_loss, loss = custom_loss(train_output, train_target, sparsity, beta1, beta2, PR, beta3, PR_connect, beta4)\n",
        "\n",
        "        # Add cross entropy loss\n",
        "        crossEntropy_loss += CE_loss\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Add train loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "      # Find the average loss across all batches for one epoch\n",
        "      avg_train_loss = train_loss / len(train_loader)\n",
        "      trainLosses.append(avg_train_loss)\n",
        "\n",
        "      avg_sparsity = sparsity_loss / len(train_loader)\n",
        "      sparsities.append(avg_sparsity)\n",
        "\n",
        "      avg_crossEntropy_loss = crossEntropy_loss/len(train_loader)\n",
        "      crossEntropyLosses.append(avg_crossEntropy_loss)\n",
        "\n",
        "      avg_PR_loss = PR_loss/len(train_loader)\n",
        "      PR_losses.append(avg_PR_loss)\n",
        "\n",
        "      avg_PR_connect_loss = PR_connect_loss/len(train_loader)\n",
        "      PR_connect_losses.append(avg_PR_connect_loss)\n",
        "\n",
        "      # Validation Stage\n",
        "      model.eval()\n",
        "      val_loss = 0\n",
        "      total = 0\n",
        "      correct = 0\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        for test_input, test_target in test_loader: # 5. LOOP FOR TEST BATCH\n",
        "\n",
        "          # test input\n",
        "          test_x_h, test_output = model(test_input)\n",
        "\n",
        "          # Take out the connectivity matrix\n",
        "          connectivity_matrix = model.input_layer.weight.data\n",
        "          PR_connect = PR_norm(connectivity_matrix)\n",
        "\n",
        "          # Compute sparsity from the hidden layer\n",
        "          sparsity = L1_norm(x_h)\n",
        "\n",
        "          # Compute participation ratio from the hidden layer\n",
        "          PR = PR_norm(x_h)\n",
        "\n",
        "          # <<Compute the total loss>>\n",
        "          _, loss = custom_loss(train_output, train_target, sparsity, beta1, beta2, PR, beta3, PR_connect, beta4)\n",
        "\n",
        "          # Validation loss\n",
        "          val_loss += loss.item()\n",
        "\n",
        "          # Get the max output?\n",
        "          _, predicted = torch.max(test_output, 1)\n",
        "\n",
        "          # Store the accuracy\n",
        "          total += test_target.size(0) # for each batch, it gets added (so basically per epoch, total should be equal to the number of total input)\n",
        "          correct += (predicted == test_target).sum().item() # same thing here\n",
        "\n",
        "      accuracy = correct/total # you are already getting the accuracy for the whole training data\n",
        "      accuracies.append(accuracy)\n",
        "\n",
        "      # Average validation loss for this epoch\n",
        "      avg_val_loss = val_loss / len(test_loader)\n",
        "      valLosses.append(avg_val_loss)\n",
        "      # if (epoch+1) % 1 == 0:\n",
        "      #   print(f'Epoch [{epoch+1}/{num_epochs}], avg_crossEntropy_loss {avg_crossEntropy_loss:.6f}, Validation Loss: {val_loss/len(test_loader):.4f}')\n",
        "\n",
        "      # Check if the average training loss is below the threshold and the epoch count is at least 100\n",
        "      if avg_crossEntropy_loss < 0.5 and epoch >= 100: #0.001\n",
        "        print(f\"Stopping training at epoch {epoch + 1} with avg_crossEntropy_loss {avg_crossEntropy_loss:.6f}\")\n",
        "        break\n",
        "      # if (epoch+1) % 1 == 0:\n",
        "      #   print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(test_loader):.4f}')\n",
        "\n",
        "    ## These are all after epochs are finished.\n",
        "    ## These get added at the end of one experiment\n",
        "    if accuracies[-1] == accuracy: # check to make sure only grab the last accuracy after all epochs\n",
        "      total_accuracies[iter, exp_list[i][0], exp_list[i][1]] = accuracy\n",
        "\n",
        "    # output of the hidden layer based on exp_testT\n",
        "    new_x_h, _ = model(shuffled_testI)\n",
        "\n",
        "    # Do PCA on the output\n",
        "    pca = PCA(n_components = 3)\n",
        "    new_x_h = new_x_h.detach().numpy()\n",
        "    transformed_Xh = pca.fit_transform(new_x_h)\n",
        "    pca_dict[i][iter,:,:] = transformed_Xh\n",
        "\n",
        "  end_time = time.time()  # End time for the epoch\n",
        "  epoch_time = end_time - start_time\n",
        "  print(f\"Epoch {epoch + 1} took {epoch_time} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43QmTIeQMrhH"
      },
      "outputs": [],
      "source": [
        "# FIND mean across iternums\n",
        "mean_total_acc = np.average(total_accuracies, axis=0)\n",
        "\n",
        "# Display the mean total acc\n",
        "plt.imshow(mean_total_acc, aspect='auto', cmap='viridis')\n",
        "plt.colorbar(label='Mean Accuracy')  # Adding a color legend with label\n",
        "\n",
        "# Setting the tick labels for x and y axes\n",
        "plt.xticks(ticks=[0, 1, 2], labels=['1', '2', '3'])\n",
        "plt.yticks(ticks=[0, 1, 2], labels=['1', '2', '3'])\n",
        "\n",
        "# Adding labels to the axes\n",
        "plt.xlabel('Test')\n",
        "plt.ylabel('Train')\n",
        "plt.title('Mean Accuracy Across Iterations')\n",
        "\n",
        "# Annotate each cell with the numeric value\n",
        "for i in range(mean_total_acc.shape[0]):\n",
        "    for j in range(mean_total_acc.shape[1]):\n",
        "        plt.text(j, i, f'{mean_total_acc[i, j]:.2f}', ha='center', va='center', color='white', fontsize=14)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "celoss = []\n",
        "for i in range(len(crossEntropyLosses)):\n",
        "  celoss.append(crossEntropyLosses[i].detach().numpy())"
      ],
      "metadata": {
        "id": "tFPPei-arxP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "celoss[-1]"
      ],
      "metadata": {
        "id": "XMl-gaOGuYAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "celoss"
      ],
      "metadata": {
        "id": "1Il_910_4rHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(celoss)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "plt.title(\"Cross Entropy Loss\")\n",
        "plt.yscale('log')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nfGC80aIrkDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sS0iLCb8NIPt"
      },
      "outputs": [],
      "source": [
        "# Mean PCA dictionary\n",
        "mean_pca_dict = {i: np.zeros((400, 3)) for i in range(len(exp_list))}\n",
        "for i in range(9):\n",
        "  mean_pca_dict[i] = np.mean(pca_dict[i], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_Y_2KLOyW16"
      },
      "outputs": [],
      "source": [
        "# Calculate the centroid\n",
        "four_points_dict = {i: np.zeros((4, 3)) for i in range(len(exp_list))}\n",
        "\n",
        "for i in range(9):\n",
        "  four_points_dict[i][0,:] = calculate_centroid(mean_pca_dict[i][:100,:])\n",
        "  four_points_dict[i][1,:] = calculate_centroid(mean_pca_dict[i][100:200,:])\n",
        "  four_points_dict[i][2,:] = calculate_centroid(mean_pca_dict[i][200:300,:])\n",
        "  four_points_dict[i][3,:] = calculate_centroid(mean_pca_dict[i][300:,:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNNBZ0Uecruz"
      },
      "outputs": [],
      "source": [
        "# SHOW ALL THE POINTS\n",
        "fig, axs = plt.subplots(3, 3, figsize=(10,10), subplot_kw={'projection': '3d'})\n",
        "# Define colors for each subplot\n",
        "colors = ['r', 'g', 'b', 'k']\n",
        "\n",
        "subplot_titles = ['Train 1, Test 1', 'Train 1, Test 2', 'Train 1, Test 3',\n",
        "                  'Train 2, Test 1', 'Train 2, Test 2', 'Train 2, Test 3',\n",
        "                  'Train 3, Test 1', 'Train 3, Test 2', 'Train 3, Test 3']\n",
        "\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    points = mean_pca_dict[i]\n",
        "    # Scatter plot the transformed data\n",
        "    for _, (idx, color) in enumerate(zip(range(4), ['r','g','b','k'])):\n",
        "        ax.scatter(points[idx*100:(idx+1)*100, 0],\n",
        "                  points[idx*100:(idx+1)*100, 1],\n",
        "                  points[idx*100:(idx+1)*100, 2],\n",
        "                  c=color, marker='o')\n",
        "\n",
        "\n",
        "    # Set labels and title (axis is important)\n",
        "    ax.set_xlabel('PC 1')\n",
        "    ax.set_ylabel('PC 2')\n",
        "    ax.set_zlabel('PC 3')\n",
        "    ax.set_title(subplot_titles[i])\n",
        "    ax.set_xlim([-15, 15])\n",
        "    ax.set_ylim([-15, 15])\n",
        "    ax.set_zlim([-15, 15])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHR-mas7wadE"
      },
      "outputs": [],
      "source": [
        "# Show the main centroid points\n",
        "fig, axs = plt.subplots(3, 3, figsize=(8,8), subplot_kw={'projection': '3d'})\n",
        "\n",
        "# Define colors for each subplot\n",
        "colors = ['r', 'g', 'b', 'k']\n",
        "\n",
        "# Plot each set of points in its respective subplot\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    points = four_points_dict[i]\n",
        "    for j in range(4):\n",
        "        ax.scatter(points[j, 0], points[j, 1], points[j, 2],\n",
        "                   c=colors[j], marker='o', label=f'Point {j}')\n",
        "    ax.set_title(subplot_titles[i])\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')\n",
        "    # ax.set_xlim([-1, 1])\n",
        "    # ax.set_ylim([-1, 1])\n",
        "    # ax.set_zlim([-1, 1])\n",
        "    ax.legend()\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3rwO_w1w6TN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTIIYTbORIxA7sDXni2/QK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}